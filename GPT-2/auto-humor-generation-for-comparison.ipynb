{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T13:52:35.222901Z",
     "start_time": "2019-12-11T13:52:35.212818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.run_this_cell {display: block !important;} </style"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.run_this_cell {hover: block !important;} </style"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width: 95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.run_this_cell {display: block !important;} </style\"))\n",
    "display(HTML(\"<style>.run_this_cell {hover: block !important;} </style\"))\n",
    "display(HTML(\"<style>.container { width: 95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T13:52:36.249212Z",
     "start_time": "2019-12-11T13:52:36.237833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users2/ejoswin/real/pytorch-transformers\n"
     ]
    }
   ],
   "source": [
    "cd /users2/ejoswin/real/pytorch-transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T13:52:38.213658Z",
     "start_time": "2019-12-11T13:52:37.366569Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T13:52:41.653203Z",
     "start_time": "2019-12-11T13:52:39.624741Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T13:52:42.125716Z",
     "start_time": "2019-12-11T13:52:42.110637Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    top_p = 0.5\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T13:52:45.008552Z",
     "start_time": "2019-12-11T13:52:44.991383Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_sequence(model, length, context, num_samples=1, temperature=1, \n",
    "                    top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
    "                    is_xlnet=False, is_xlm_mlm=False, xlm_mask_token=None,\n",
    "                    xlm_lang=None, device='cpu'):\n",
    "    \n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    generated = context\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "#             print('i', _)\n",
    "            inputs = {'input_ids': generated}\n",
    "\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "\n",
    "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
    "            for i in range(num_samples):\n",
    "                for _ in set(generated[i].tolist()):\n",
    "                    next_token_logits[i, _] /= repetition_penalty\n",
    "                \n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            if temperature == 0: # greedy sampling:\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T13:52:47.200283Z",
     "start_time": "2019-12-11T13:52:47.186747Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def obtain_output(model, tokenizer, prompts, end='<|endoftext|>'):\n",
    "    model_outputs = []\n",
    "    for i in range(len(prompts)):\n",
    "        prompt = prompts[i]\n",
    "\n",
    "        context_tokens = tokenizer.encode(prompt)\n",
    "        length = 40\n",
    "        temperature = 0.3\n",
    "        top_k = 50\n",
    "        top_p = 1.0\n",
    "        repetition_penalty = 1.0\n",
    "        device = torch.device(\"cuda\")\n",
    "\n",
    "        out = sample_sequence(\n",
    "                    model=model,\n",
    "                    context=context_tokens,\n",
    "                    num_samples=1,\n",
    "                    length=length,\n",
    "                    temperature=temperature,\n",
    "                    top_k=top_k,\n",
    "                    top_p=top_p,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                    device=\"cuda\",)\n",
    "\n",
    "        out = out[: len(context_tokens):].tolist()\n",
    "        for o in out:\n",
    "            text = tokenizer.decode(o, clean_up_tokenization_spaces=True)\n",
    "            text = text[: text.find(end)]\n",
    "\n",
    "        model_outputs.append(text)\n",
    "        \n",
    "    return model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T13:52:53.972338Z",
     "start_time": "2019-12-11T13:52:49.503284Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_tuned_model_path = '/data/ejoswin/data/distillgpt2_joke_epochs_10/output/'\n",
    "\n",
    "model_fine_tuned = GPT2LMHeadModel.from_pretrained(fine_tuned_model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T13:53:01.319250Z",
     "start_time": "2019-12-11T13:52:56.979185Z"
    }
   },
   "outputs": [],
   "source": [
    "model_pre_trained_path = 'distilgpt2'\n",
    "model_pre_trained = GPT2LMHeadModel.from_pretrained(model_pre_trained_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_pre_trained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T13:55:00.902605Z",
     "start_time": "2019-12-11T13:55:00.885998Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"When did Obama become\",\n",
    "\"What do you call a bee\",\n",
    "\"Who is a man who\",\n",
    "\"I love NLP even though\",\n",
    "\"There has never been a time\",\n",
    "\"Though shalt not brush your teeth\",\n",
    "\"Who does a chair belong\",\n",
    "\"Do you know what happened to\",\n",
    "\"Your mommma is so fat that\",\n",
    "\"Who let the dogs out\",\n",
    "\"When did Emil\",\n",
    "\"Aditya has a\",\n",
    "\"When did the\",\n",
    "\"What did the nun\",\n",
    "\"She has announced\",\n",
    "\"Why did he\",\n",
    "\"Why did she\",\n",
    "\"The polic pulled\",\n",
    "\"Who is called\",\n",
    "\"Why did you\",\n",
    "\"This is not\",\n",
    "\"How did Mary\",\n",
    "\"How did John\",\n",
    "\"When did Obama become the famous\",\n",
    "\"What do you call a bee who does not knock\",\n",
    "\"Who is a man who does not pay his\",\n",
    "\"Who let the dogs out in the garden\",\n",
    "\"I love NLP even though it has a\",\n",
    "\"There has never been a time when the price of\",\n",
    "\"Though shalt not brush your teeth\",\n",
    "\"Who does a chair belong to if he has\",\n",
    "\"Do you know what happened to\",\n",
    "\"Your mommma is so fat that when she\",\n",
    "\"A doctor accidentally prescribes\",\n",
    "\"An old grandma brings a\",\n",
    "\"I got another letter \",\n",
    "\"Can you please hold my\",\n",
    "\"A husband and a wife sit at the\",\n",
    "\"One of the most wonderful things in\",\n",
    "\"I went to the zoo \",\n",
    "\"Take the spoon out of the\",\n",
    "\"I really appreciate your\",\n",
    "\"I am really sorry for\",\n",
    "\"She prayed for her brother\",\n",
    "\"Karam was very unhappy because\",\n",
    "\"Take effective measure to stop\",\n",
    "\"Allen posted a new \",\n",
    "\"How to build a \",\n",
    "\"The black hole can serve as a\",\n",
    "\"The list of sentences is\",\n",
    "\"This means that even advanced learners\",\n",
    "\"The frog jumped and landed\",\n",
    "\"I am out of paper for the\",\n",
    "\"The music is too loud for my\",\n",
    "\"Can I have some juice to\",\n",
    "\"There is a fly in the \",\n",
    "\"The definitions are clear and clarify the\",\n",
    "\"John's family is renovating\",\n",
    "\"A bunch of dogs are in the\",\n",
    "\"The third factor is the amount of\",\n",
    "\"Use competing products to recommend\",\n",
    "\"We are from the planet\",\n",
    "\"Oil company stock prices \",\n",
    "\"We asked our trained model to\",\n",
    "\"The fact that he smiled at me gives me\",\n",
    "\"All sentences beginning with a\",\n",
    "\"Did you hear about the mathematician\",\n",
    "\"To fit and\",\n",
    "\"How do you make your girlfriend\",\n",
    "\"Why does a mermaid wear\",\n",
    "\"A penguin takes his car\",\n",
    "\"A Chinese girl asked\",\n",
    "\"My dog used to chase\",\n",
    "\"Arnold and Juan play\",\n",
    "\"The staff performed\",\n",
    "\"He sold it for a high price on\",\n",
    "\"Extremely Loud and\",\n",
    "\"Early to rise and early to\",\n",
    "\"The paper and pencil sat idle on the\",\n",
    "\"Misha walked and looked\",\n",
    "\"Neither boy spoke\",\n",
    "\"The ham, green beans,\",\n",
    "\"Joe stood up and spoke to the\",\n",
    "\"This new laptop computer has\",\n",
    "\"Iâ€˜m happy, but my\",\n",
    "\"Stop! Close the\",\n",
    "\"How exciting the movie\",\n",
    "\"My mom cooked pancakes for\",\n",
    "\"Syrup comes from\",\n",
    "\"The big dog and the small cat went\",\n",
    "\"The fox was great at\",\n",
    "\"He obtained his degree\",\n",
    "\"After the death of the \",\n",
    "\"Nothing beats a complete\",\n",
    "\"The Taskmaster corpus consists of various spoken and written\",\n",
    "\"Did you know Rahul have a\",\n",
    "\"We removed the most common\",\n",
    "\"As long as you remember to include these\",\n",
    "\"Brad came to dinner with\",\n",
    "\"Isn't language learning\",\n",
    "\"An exclamation mark indicates an\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T13:55:26.670029Z",
     "start_time": "2019-12-11T13:55:26.658098Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4-6 \n",
    "prompts_1 = [\"When did Obama become\", \"What do you call a bee \", \"Who is a man who\", \\\n",
    "             \"I love NLP even though\", \"There has never been a time\", \"Though shalt not brush your teeth\", \\\n",
    "            \"Who does a chair belong\", \"Do you know what happened to\", \"Your mommma is so fat that\", \\\n",
    "            \"Who let the dogs out\", ]\n",
    "\n",
    "# 3-4\n",
    "prompts_2 = [\"When did Emil\", \"Aditya has a \", \"When did the\", \"What did the nun\", \"She has announced\", \\\n",
    "            \"Why did he\", \"Why did she\", \"The polic pulled\", \"Who is called\", \"Why did you\", \"This is not\",\n",
    "            \"How did Mary\", \"How did John\"]\n",
    "\n",
    "# 8-10\n",
    "prompts_3 = [\"When did Obama become the famous\", \"What do you call a bee who does not knock\", \\\n",
    "             \"Who is a man who does not pay his\", \"Who let the dogs out in the garden\", \\\n",
    "             \"I love NLP even though it has a \", \"There has never been a time when the price of\" , \\\n",
    "             \"Though shalt not brush your teeth\", \"Who does a chair belong to if he has\", \n",
    "             \"Do you know what happened to\", \"Your mommma is so fat that when she\", \\\n",
    "         ]\n",
    "\n",
    "# model = model_fine_tuned.cuda()\n",
    "\n",
    "def call_gpt(model, tokenizer, prompt, _type='joke'):\n",
    "    model = model.cuda()\n",
    "\n",
    "    if _type == 'joke':\n",
    "        outputs = obtain_output(model, tokenizer, prompt)\n",
    "    else:\n",
    "        outputs = obtain_output(model, tokenizer, prompt, end='\\n')\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T18:07:53.805319Z",
     "start_time": "2019-12-10T18:07:53.797145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 13, 10]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(p) for p in [prompts_1, prompts_2, prompts_3] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T14:39:16.926720Z",
     "start_time": "2019-12-11T14:38:31.292772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    }
   ],
   "source": [
    "all_jokes = call_gpt(model_fine_tuned, tokenizer, prompts, _type='nojoke')\n",
    "all_no_jokes = call_gpt(model_pre_trained, tokenizer, prompts, _type='nojoke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T14:40:47.465757Z",
     "start_time": "2019-12-11T14:40:47.459366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 101, 101)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_prompts = prompts\n",
    "len(all_prompts), len(all_jokes), len(all_no_jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T14:27:54.866401Z",
     "start_time": "2019-12-11T14:27:19.778181Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 When did Obama become\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 21 21\n",
      "1 What do you call a bee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1a6728e7efd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mjokes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fine_tuned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'joke'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnojokes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pre_trained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nojoke'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnojokes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjokes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-9b4f22321c2d>\u001b[0m in \u001b[0;36mcall_gpt\u001b[0;34m(model, tokenizer, prompt, _type)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobtain_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobtain_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-dac618bdb050>\u001b[0m in \u001b[0;36mobtain_output\u001b[0;34m(model, tokenizer, prompts, end)\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     device=\"cuda\",)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d1eea9512a7e>\u001b[0m in \u001b[0;36msample_sequence\u001b[0;34m(model, length, context, num_samples, temperature, top_k, top_p, repetition_penalty, is_xlnet, is_xlm_mlm, xlm_mask_token, xlm_lang, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mnext_token_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mfiltered_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_k_top_p_filtering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# greedy sampling:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mnext_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-dca885ba6ffa>\u001b[0m in \u001b[0;36mtop_k_top_p_filtering\u001b[0;34m(logits, top_k, top_p, filter_value)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# scatter sorted tensors to original indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mindices_to_remove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_indices_to_remove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted_indices_to_remove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices_to_remove\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_prompts = []\n",
    "all_jokes = []\n",
    "all_no_jokes = []\n",
    "\n",
    "# for p in list([prompts_1, prompts_2, prompts_3]):\n",
    "for i, p in enumerate(prompts):\n",
    "    print(i, p)\n",
    "\n",
    "\n",
    "    jokes = call_gpt(model_fine_tuned, tokenizer, p, _type='joke')\n",
    "    nojokes = call_gpt(model_pre_trained, tokenizer, p, _type='nojoke')\n",
    "    \n",
    "    print( len(p), len(nojokes), len(jokes) )\n",
    "    \n",
    "    all_jokes += jokes\n",
    "    all_no_jokes += nojokes\n",
    "    all_prompts += p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T14:40:50.760638Z",
     "start_time": "2019-12-11T14:40:50.753586Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 101, 101)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_prompts), len(all_no_jokes), len(all_jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T14:40:53.141616Z",
     "start_time": "2019-12-11T14:40:53.123631Z"
    }
   },
   "outputs": [],
   "source": [
    "lengths = [len(i.split()) for i in all_prompts]\n",
    "\n",
    "df = pd.DataFrame({'prompt': all_prompts, 'joke': all_jokes, 'no_jokes': all_no_jokes, 'length': lengths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T14:40:54.523642Z",
     "start_time": "2019-12-11T14:40:54.497473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>joke</th>\n",
       "      <th>no_jokes</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>When did Obama become</td>\n",
       "      <td>When did Obama become president? When he was i...</td>\n",
       "      <td>When did Obama become the first president to h...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>What do you call a bee</td>\n",
       "      <td>What do you call a bee that's been hit by a ca...</td>\n",
       "      <td>What do you call a beekeeper?</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Who is a man who</td>\n",
       "      <td>Who is a man who can't get a girl pregnant? A ...</td>\n",
       "      <td>Who is a man who is a man who is a man who is ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>I love NLP even though</td>\n",
       "      <td>I love NLP even though I'm not a fan of Nemonos.</td>\n",
       "      <td>I love NLP even though I don't know what it is...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>There has never been a time</td>\n",
       "      <td>There has never been a time when I was a kid a...</td>\n",
       "      <td>There has never been a time for a man to be a ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>We removed the most common</td>\n",
       "      <td>We removed the most common word in the English...</td>\n",
       "      <td>We removed the most common forms of the diseas...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>As long as you remember to include these</td>\n",
       "      <td>As long as you remember to include these words...</td>\n",
       "      <td>As long as you remember to include these in yo...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>Brad came to dinner with</td>\n",
       "      <td>Brad came to dinner with a note that said: I'm...</td>\n",
       "      <td>Brad came to dinner with the family.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>Isn't language learning</td>\n",
       "      <td>Isn't language learning like being a prostitut...</td>\n",
       "      <td>Isn't language learning is a good thing. It's ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>An exclamation mark indicates an</td>\n",
       "      <td>An exclamation mark indicates an offender is n...</td>\n",
       "      <td>An exclamation mark indicates an exclamation m...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       prompt  \\\n",
       "0                       When did Obama become   \n",
       "1                      What do you call a bee   \n",
       "2                            Who is a man who   \n",
       "3                      I love NLP even though   \n",
       "4                 There has never been a time   \n",
       "..                                        ...   \n",
       "96                 We removed the most common   \n",
       "97   As long as you remember to include these   \n",
       "98                   Brad came to dinner with   \n",
       "99                    Isn't language learning   \n",
       "100          An exclamation mark indicates an   \n",
       "\n",
       "                                                  joke  \\\n",
       "0    When did Obama become president? When he was i...   \n",
       "1    What do you call a bee that's been hit by a ca...   \n",
       "2    Who is a man who can't get a girl pregnant? A ...   \n",
       "3     I love NLP even though I'm not a fan of Nemonos.   \n",
       "4    There has never been a time when I was a kid a...   \n",
       "..                                                 ...   \n",
       "96   We removed the most common word in the English...   \n",
       "97   As long as you remember to include these words...   \n",
       "98   Brad came to dinner with a note that said: I'm...   \n",
       "99   Isn't language learning like being a prostitut...   \n",
       "100  An exclamation mark indicates an offender is n...   \n",
       "\n",
       "                                              no_jokes  length  \n",
       "0    When did Obama become the first president to h...       4  \n",
       "1                        What do you call a beekeeper?       6  \n",
       "2    Who is a man who is a man who is a man who is ...       5  \n",
       "3    I love NLP even though I don't know what it is...       5  \n",
       "4    There has never been a time for a man to be a ...       6  \n",
       "..                                                 ...     ...  \n",
       "96   We removed the most common forms of the diseas...       5  \n",
       "97   As long as you remember to include these in yo...       8  \n",
       "98                Brad came to dinner with the family.       5  \n",
       "99   Isn't language learning is a good thing. It's ...       3  \n",
       "100  An exclamation mark indicates an exclamation m...       5  \n",
       "\n",
       "[101 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T14:41:05.682058Z",
     "start_time": "2019-12-11T14:41:05.673087Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('/users2/ejoswin/notebooks/NLP-project/final3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
